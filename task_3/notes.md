# Responsible AI in Speech and Audio Processing by Cristian Tejedor-Garc√≠a
[...]
Responsible AI for Voice Diagnostics project (RAIVD)
Focusing on neurodegenerative deseases and education
PhD positions are available
1. Advances are underway 
2.  Ttransformers will be recplaed by continuous learning
3.  ...
4.  ...

Conclusion
1. Clarity on Speech Data Classification
2. Privacy by Design & Default
3. Data Subjects Rights Enforcement
4. Privacy-Preserving Methods
5. EU Regulatory Gaps

Can you request data from Amazon under the GDPR that has been processed in the Alexa system?
When you use devices like Alexa, you are selling yourself. We don't know what exactly is being processed. There are rules but they aren't enforced

(...?)
ASR is not the same as text. We cannot apply (...) to audio in the same way we do for text.

How effective do you think pseudoanonymization will be for ASR privacy?
For the company, it works really well, but for the user, it doesn't. It's a balance: you use their services, they get your voice, and you cannot control what they do with your voice

How do you work? How do you do your research?
We work with children and the elderly. The data is sent to our cluster at the RU. They need to sign a form of consent. If they want the data removed, we agree (we are not a company). 
There are these deepfake videos on YouTube with models that only need 1 or 2 minutes of your voice in order to replicate you.

What is the education part about for the RAIVD project about?
Diagnosing speech disabilities in children, we want to report that to their environment. Also for improving language learning for non-natives.

Everything rests on these big models you don't control/own, can that be considered responsible? Is there anything you can do?
Most of these models are open source. But when you use an application, you are sending your data over the internet to a company. That part is relevant for RAI in our research. We are developing an LLM for Dutch and there are companies that want to make use of it. Our license will be free for universities, but not for companies.

If it's so easy to generate speech that sounds like me nowadays, then why should I care that my voice is being used?
Companies that use your voice should implement good security measures in order to protect your voice data.

Are we too late already?
We are too late for not making Google etc. as rich as they are, but not too late for making and assessing laws protecting our voices. There isn't a lot of research about voice privacy, I hope the community decides to research it more.


